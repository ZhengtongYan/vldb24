#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
from math import prod
import math
import itertools
from math import inf
from sympy.utilities.iterables import multiset_permutations
import Scripts.QUBOGenerator as QUBOGenerator
import Scripts.ProblemGenerator as ProblemGenerator
import Scripts.Postprocessing as Postprocessing
import Scripts.DataExport as DataExport

import json
import os
import pathlib
import csv
from os import listdir
from os.path import isfile, join
from pathlib import Path

from multiprocessing import Pool
from multiprocessing.pool import ThreadPool

import time

from dadk.QUBOSolverDAv2 import QUBOSolverDAv2
from dadk.QUBOSolverCPU import *


# In[2]:


def save_to_csv(data, path, filename):
    sd = os.path.abspath(path)
    pathlib.Path(sd).mkdir(parents=True, exist_ok=True) 
    
    f = open(path + '/' + filename, 'a', newline='')
    writer = csv.writer(f)
    writer.writerow(data)
    f.close()


def load_data(path, filename):
    datafile = os.path.abspath(path + '/' + filename)
    if os.path.exists(datafile):
        with open(datafile, 'rb') as file:
            return json.load(file)
        
def load_all_results(path):
    if not os.path.isdir(path):
        return []
    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]
    data = []
    for datafile in onlyfiles:
        with open(path + '/' + datafile, 'rb') as file:
            data.append(json.load(file))
    return data

def save_data(data, path, filename):

    datapath = os.path.abspath(path)
    pathlib.Path(datapath).mkdir(parents=True, exist_ok=True) 
    
    datafile = os.path.abspath(path + '/' + filename)
    mode = 'a' if os.path.exists(datafile) else 'w'
    with open(datafile, mode) as file:
        json.dump(data, file)


# In[3]:


def solve_problem(fujitsu_qubo, da_algorithm='annealing', number_runs=100, number_iterations=1000000, test_with_local_solver=False):
    if test_with_local_solver:
        solver = QUBOSolverCPU(number_runs=number_runs)
    else:
        if da_algorithm == 'annealing':
            solver = QUBOSolverDAv2(optimization_method=da_algorithm, timeout=60, number_iterations=number_iterations, number_runs=number_runs, access_profile_file='annealer.prf', use_access_profile=True)
        else:
            solver = QUBOSolverDAv2(optimization_method=da_algorithm, timeout=60, number_iterations=number_iterations, number_replicas=number_runs, access_profile_file='annealer.prf', use_access_profile=True)
    
    
    while True:
        try:
            solution_list = solver.minimize(fujitsu_qubo)
            break
        except:
            print("Library error. Repeating request")

    execution_time = solution_list.execution_time.total_seconds()
    anneal_time = solution_list.anneal_time.total_seconds()
    solutions = solution_list.solutions
    return solutions, execution_time, anneal_time

def parse_solutions_for_serialisation(raw_solutions):
    response = []
    for raw_solution in raw_solutions:
        solution = [raw_solution.configuration, float(raw_solution.frequency), float(raw_solution.energy)]
        response.append(solution)
    return response

def get_algorithm_name(da_algorithm):
    if da_algorithm == 'annealing':
        return 'digital_annealing'
    if da_algorithm == 'parallel_tempering':
        return 'parallel_tempering'

def get_combined_cost_evolution(annealing_thres_results, best_thres_config):
    combined_results = []
    for (k, v) in annealing_thres_results.items():
        if list(k)[1] == best_thres_config:
            combined_results.extend(v)

    combined_results = sorted(combined_results, key=lambda x: x['time'])

    min_costs = inf
    filtered_result = []
    while len(combined_results) != 0:
        combined_result = combined_results.pop(0)
        if combined_result["costs"] < min_costs:
            min_costs = combined_result["costs"]
            filtered_result.append(combined_result)
    
    return filtered_result

def conduct_benchmark_experiments(da_algorithms, approximation_types, penalty_scalings, iterations_list, approximation_precisions, problem_path_prefix, result_path_prefix, number_runs=100, max_num_samples=20, timeout_in_ms=60000):
    benchmarks = os.listdir(path=problem_path_prefix + '/benchmarks')
    for benchmark in benchmarks:
        queries = os.listdir(path=problem_path_prefix + '/benchmarks/' + benchmark)
        for query in queries:
            query_number = int(query.split('q')[1])
            card, pred, pred_sel = ProblemGenerator.get_join_ordering_problem(problem_path_prefix + '/benchmarks/' + benchmark + '/' + query, generated_problems=True)
            
            if 0.0 in pred_sel:
                continue
            for da_algorithm in da_algorithms:
                for penalty_scaling in penalty_scalings:
                    for l in range(len(approximation_precisions)):
                        (ap, num_decimal_pos, thres) = approximation_precisions[l]
                        for approximation_type in approximation_types:
                            if approximation_type == 'quadratic':
                                if len(thres) == 0:
                                    fujitsu_qubo, penalty_weight = QUBOGenerator.generate_Fujitsu_QUBO_for_left_deep_trees_v2(card, pred, pred_sel, penalty_scaling=penalty_scaling)
                                else:
                                    fujitsu_qubo, penalty_weight = QUBOGenerator.generate_Fujitsu_QUBO_for_left_deep_trees(card, pred, pred_sel, thres[0], num_decimal_pos, penalty_scaling=penalty_scaling)
                            elif approximation_type == 'legacy':
                                fujitsu_qubo, penalty_weight = QUBOGenerator.generate_Fujitsu_legacy_QUBO_for_left_deep_trees(card, pred, pred_sel, thres, num_decimal_pos, penalty_scaling=penalty_scaling)
                            for number_iterations in iterations_list:
                                total_annealing_time = 0
                                sample_index = 0
                                while sample_index < max_num_samples and total_annealing_time < timeout_in_ms:
                                    result_path = result_path_prefix + '/da/benchmarks/' + benchmark + '/' + query + '/' + approximation_type + '_approximation' + '/thres_config_' + str(ap) + '/penalty_scaling_' + str(penalty_scaling) + '/' + da_algorithm + '/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + 'sample_' + str(sample_index)
                                    raw_solutions, execution_time, anneal_time = solve_problem(fujitsu_qubo, da_algorithm=da_algorithm, number_runs=number_runs, number_iterations=number_iterations)
                                    solutions = parse_solutions_for_serialisation(raw_solutions)
                                    total_annealing_time = total_annealing_time + (execution_time * 1000)
                                    DataExport.compress_and_save_data([solutions, float(execution_time), float(anneal_time)], result_path, "response.txt") 
                                    thres_path = result_path_prefix + '/da/benchmarks/' + benchmark + '/' + query + '/' + approximation_type + '_approximation' + '/thres_config_' + str(ap)
                                    if not os.path.exists(thres_path + '/thres_config.txt'):
                                        save_data(thres, thres_path, 'thres_config.txt')
                                    sample_index = sample_index + 1 
                                
def process_benchmark_annealing_results(da_algorithms, approximation_types, penalty_scalings, iterations_list, considered_thres_configs, problem_path_prefix, data_path_prefix, result_path_prefix, number_runs=100, max_num_samples=20, replace_existing_results=False, timeout_in_ms=60000):
    benchmarks = os.listdir(path=problem_path_prefix + '/benchmarks')
    for benchmark in benchmarks:
        queries = os.listdir(path=problem_path_prefix + '/benchmarks/' + benchmark)
        for query in queries:
            query_number = int(query.split('q')[1])
            card, pred, pred_sel = ProblemGenerator.get_join_ordering_problem(problem_path_prefix + '/benchmarks/' + benchmark + '/' + query, generated_problems=True)
            if 0.0 in pred_sel:
                continue
            card_dict = {}
            for da_algorithm in da_algorithms:
                for approximation_type in approximation_types:
                    for penalty_scaling in penalty_scalings:
                        annealing_thres_results = {}
                        min_cost = inf
                        best_config = None
                        for number_iterations in iterations_list:
                            result_path = result_path_prefix + '/benchmarks/' + benchmark + '/' + query 
                            result_file = result_path + '/' + get_algorithm_name(da_algorithm) + '_' + approximation_type + '.json'
                            if os.path.exists(result_file):
                                if replace_existing_results:
                                    try:
                                        os.remove(result_file)
                                    except OSError:
                                        pass
                                else:
                                    continue

                            thres_config_path = data_path_prefix + '/da/benchmarks/' + benchmark + '/' + query + '/' + approximation_type + '_approximation'
                            if not os.path.exists(thres_config_path):
                                continue
                            thres_configs = os.listdir(path=thres_config_path)
                            for thres_config in thres_configs:
                                if considered_thres_configs is not None and thres_config not in considered_thres_configs:
                                    continue
                                if best_config is None:
                                    best_config = frozenset([number_iterations, thres_config])

                                annealing_thres_results[frozenset([number_iterations, thres_config])] = []

                                solution_time = 0
                                min_annealing_cost = inf
                                min_annealing_result = None
                                for s in range(max_num_samples):
                                    data_path_suffix = 'sample_' + str(s)
                                    data_path = data_path_prefix + '/da/benchmarks/' + benchmark + '/' + query + '/' + approximation_type + '_approximation/' + thres_config + '/penalty_scaling_' + str(penalty_scaling) + '/' + da_algorithm + '/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + data_path_suffix 
                                    if not os.path.exists(data_path):
                                        continue
                                    response = DataExport.load_compressed_data(data_path, "response.txt")
                                    if response is None:
                                        continue
                                    solution_time = solution_time + (response[1]*1000)
                                    best_solutions_for_time, solutions = Postprocessing.readout(response, card, pred, pred_sel, card_dict)
                                    final_solution = best_solutions_for_time[len(best_solutions_for_time)-1]
                                    
                                    join_order = [int(x) for x in final_solution[0]]
                                    annealing_cost = final_solution[1]
                                    readout_time = final_solution[2]
                                    postprocessed = final_solution[3]
                                    total_time = solution_time + readout_time
                                    if annealing_cost >= min_annealing_cost or solution_time > timeout_in_ms:
                                        continue
                                        
                                    min_annealing_cost = annealing_cost
                                    annealing_thres_results[frozenset([number_iterations, thres_config])].append({"time": total_time, "join_order":join_order, "costs": annealing_cost, "fallback": postprocessed})
                                    if annealing_cost < min_cost:
                                        min_cost = annealing_cost
                                        best_config = frozenset([number_iterations, thres_config])

                        if best_config is None:
                            save_data([], result_path, get_algorithm_name(da_algorithm) + '_' + approximation_type + '.json')
                        else:
                            best_annealing_results = get_combined_cost_evolution(annealing_thres_results, list(best_config)[1])
                            save_data(best_annealing_results, result_path, get_algorithm_name(da_algorithm) + '_' + approximation_type + '.json')
                                                
def conduct_synthetic_annealing_experiments(da_algorithms, approximation_types, penalty_scalings, iterations_list, approximation_precisions, problem_path_prefix, result_path_prefix, number_runs=100, max_num_samples=20, timeout_in_ms=60000):
    
    graph_types = os.listdir(path=problem_path_prefix + '/synthetic/')
    for graph_type_string in graph_types:
        graph_type = graph_type_string.split("_")[0]
        relations = os.listdir(path=problem_path_prefix + '/synthetic/' + graph_type + '_graph')
        for relations_string in relations:
            i = int(relations_string.split("relations")[0])
            problems = os.listdir(path=problem_path_prefix + '/synthetic/' + graph_type + '_graph/' + str(i) + 'relations')
            for j in problems:
                j = int(j)

                problem_path_main = graph_type + '_graph/' + str(i) + 'relations/' + str(j)
                card, pred, pred_sel = ProblemGenerator.get_join_ordering_problem(problem_path_prefix + '/synthetic/' + problem_path_main, generated_problems=True)

                if 0.0 in pred_sel:
                    continue
                for da_algorithm in da_algorithms:
                    for penalty_scaling in penalty_scalings:
                        for l in range(len(approximation_precisions)):
                            (ap, num_decimal_pos, thres) = approximation_precisions[l]
                            for approximation_type in approximation_types:
                                if approximation_type == 'quadratic':
                                    if len(thres) == 0:
                                        fujitsu_qubo, penalty_weight = QUBOGenerator.generate_Fujitsu_QUBO_for_left_deep_trees_v2(card, pred, pred_sel, penalty_scaling=penalty_scaling)
                                    else:
                                        fujitsu_qubo, penalty_weight = QUBOGenerator.generate_Fujitsu_QUBO_for_left_deep_trees(card, pred, pred_sel, thres[0], num_decimal_pos, penalty_scaling=penalty_scaling)
                                elif approximation_type == 'legacy':
                                    fujitsu_qubo, penalty_weight = QUBOGenerator.generate_Fujitsu_legacy_QUBO_for_left_deep_trees(card, pred, pred_sel, thres, num_decimal_pos, penalty_scaling=penalty_scaling)
                                
                                for number_iterations in iterations_list:
                                    total_annealing_time = 0
                                    sample_index = 0
                                    while sample_index < max_num_samples and total_annealing_time < timeout_in_ms:
                                        result_path = result_path_prefix + '/da/synthetic/' + graph_type_string + '/' + relations_string + '/' + str(j) + '/' + approximation_type + '_approximation' + '/thres_config_' + str(ap) + '/penalty_scaling_' + str(penalty_scaling) + '/' + da_algorithm + '/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + 'sample_' + str(sample_index)

                                        raw_solutions, execution_time, anneal_time = solve_problem(fujitsu_qubo, da_algorithm=da_algorithm, number_runs=number_runs, number_iterations=number_iterations)
                                        solutions = parse_solutions_for_serialisation(raw_solutions)
                                        total_annealing_time = total_annealing_time + (execution_time * 1000)
                                        save_data([solutions, float(execution_time), float(anneal_time)], result_path, "response.txt") 
                                        thres_path = result_path_prefix + '/da/synthetic/' + graph_type_string + '/' + relations_string + '/' + str(j) + '/' + approximation_type + '_approximation' + '/thres_config_' + str(ap)
                                        if not os.path.exists(thres_path + '/thres_config.txt'):
                                            save_data(thres, thres_path, 'thres_config.txt')
                                        sample_index = sample_index + 1
                                        

                               
def process_synthetic_annealing_results(da_algorithms, approximation_types, penalty_scalings, iterations_list, considered_thres_configs, problem_path_prefix, data_path_prefix, result_path_prefix, number_runs=100, max_num_samples=20, replace_existing_results=False, timeout_in_ms=60000):
    graph_types = os.listdir(path=problem_path_prefix + '/synthetic/')
    for graph_type_string in graph_types:
        graph_type = graph_type_string.split("_")[0]
        relations = os.listdir(path=problem_path_prefix + '/synthetic/' + graph_type + '_graph')
        for relations_string in relations:
            i = int(relations_string.split("relations")[0])
            problems = os.listdir(path=problem_path_prefix + '/synthetic/' + graph_type + '_graph/' + str(i) + 'relations')
            for j in problems:
                j = int(j)
                
                problem_path_main = graph_type + '_graph/' + str(i) + 'relations/' + str(j)
                card, pred, pred_sel = ProblemGenerator.get_join_ordering_problem(problem_path_prefix + '/synthetic/' + problem_path_main, generated_problems=True)
                
                if 0.0 in pred_sel:
                    continue
                card_dict = {}
                for da_algorithm in da_algorithms:
                    for approximation_type in approximation_types:
                        for penalty_scaling in penalty_scalings:
                            annealing_thres_results = {}
                            min_cost = inf
                            best_config = None
                            for number_iterations in iterations_list:
                                result_path = result_path_prefix + '/synthetic/' + graph_type + '_graph/' + str(i) + 'relations/' + str(j)
                                result_file = result_path + '/' + get_algorithm_name(da_algorithm) + '_' + approximation_type + '.json'
                                if os.path.exists(result_file):
                                    if replace_existing_results:
                                        try:
                                            os.remove(result_file)
                                        except OSError:
                                            pass
                                    else:
                                        continue

                                thres_config_path = data_path_prefix + '/da/synthetic/' + problem_path_main + '/' + approximation_type + '_approximation'
                                if not os.path.exists(thres_config_path):
                                    continue
                                thres_configs = os.listdir(path=thres_config_path)
                                for thres_config in thres_configs:
                                    if considered_thres_configs is not None and thres_config not in considered_thres_configs:
                                        continue
                                    if best_config is None:
                                        best_config = frozenset([number_iterations, thres_config])

                                    annealing_thres_results[frozenset([number_iterations, thres_config])] = []

                                    solution_time = 0
                                    min_annealing_cost = inf
                                    min_annealing_result = None
                                    for s in range(max_num_samples):
                                        data_path_suffix = 'sample_' + str(s)
                                        data_path = data_path_prefix + '/da/synthetic/' + problem_path_main + '/' + approximation_type + '_approximation/' + thres_config + '/penalty_scaling_' + str(penalty_scaling) + '/' + da_algorithm + '/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + data_path_suffix 
                                        if not os.path.exists(data_path):
                                            continue
                                        response = load_data(data_path, "response.txt")
                                        if response is None:
                                            continue
                                        solution_time = solution_time + (response[1]*1000)
                                        best_solutions_for_time, solutions = Postprocessing.readout(response, card, pred, pred_sel, card_dict)
                                        final_solution = best_solutions_for_time[len(best_solutions_for_time)-1]

                                        join_order = [int(x) for x in final_solution[0]]
                                        annealing_cost = final_solution[1]
                                        readout_time = final_solution[2]
                                        postprocessed = final_solution[3]
                                        total_time = solution_time + readout_time
                                        if annealing_cost >= min_annealing_cost or total_time > timeout_in_ms:
                                            continue

                                        min_annealing_cost = annealing_cost

                                        annealing_thres_results[frozenset([number_iterations, thres_config])].append({"time": total_time, "join_order":join_order, "costs": annealing_cost, "fallback": postprocessed})
                                       
                                        if annealing_cost < min_cost:
                                            min_cost = annealing_cost
                                            best_config = frozenset([number_iterations, thres_config])

                            #best_annealing_results = annealing_thres_results[best_config]
                            if best_config is None:
                                save_data([], result_path, get_algorithm_name(da_algorithm) + '_' + approximation_type + '.json')
                            else:
                                best_annealing_results = get_combined_cost_evolution(annealing_thres_results, list(best_config)[1])
                                save_data(best_annealing_results, result_path, get_algorithm_name(da_algorithm) + '_' + approximation_type + '.json')


# In[4]:


if __name__ == "__main__":
    da_algorithms = ['annealing']
    penalty_scalings = [2]
    iterations_list = [1000, 1000000]
    number_runs = 100
    max_num_samples = 10
    timeout_in_ms = 60000
    problem_path_prefix = 'Experiments/Problems'
    result_path_prefix = 'Experiments/Data'
    approximation_types = ['quadratic']
    approximation_precisions = [(4, 2, [0.63]), (5, 2, [2.55]), (6, 2, [5.11]), (12, 0, [])]
    conduct_synthetic_annealing_experiments(da_algorithms, approximation_types, penalty_scalings, iterations_list, approximation_precisions, problem_path_prefix, result_path_prefix, number_runs=number_runs, max_num_samples=max_num_samples, timeout_in_ms=timeout_in_ms) 

    approximation_types = ['legacy']
    approximation_precisions = [(4, 2, [0.63]), (5, 2, [2.55]), (6, 2, [5.11])]
    conduct_synthetic_annealing_experiments(da_algorithms, approximation_types, penalty_scalings, iterations_list, approximation_precisions, problem_path_prefix, result_path_prefix, number_runs=number_runs, max_num_samples=max_num_samples, timeout_in_ms=timeout_in_ms)    
    
    
    considered_thres_configs = ['thres_config_4', 'thres_config_5', 'thres_config_6', 'thres_config_12']
    data_path_prefix = 'Experiments/Data'
    result_path_prefix = 'Experiments/Results'
    replace_existing_results = True
    approximation_types = ['quadratic', 'legacy']
    process_synthetic_annealing_results(da_algorithms, approximation_types, penalty_scalings, iterations_list, considered_thres_configs, problem_path_prefix, data_path_prefix, result_path_prefix, number_runs=number_runs, max_num_samples=max_num_samples, replace_existing_results=replace_existing_results, timeout_in_ms=timeout_in_ms)
    
    da_algorithms = ['annealing']
    penalty_scalings = [2]
    iterations_list = [1000, 1000000]
    number_runs = 100
    max_num_samples = 10
    timeout_in_ms = 60000
    problem_path_prefix = 'Experiments/Problems'
    result_path_prefix = 'Experimentals/Data'
    approximation_types = ['quadratic']
    approximation_precisions = [(4, 2, [0.63]), (5, 2, [2.55]), (6, 2, [5.11]), (12, 0, [])]
    conduct_benchmark_experiments(da_algorithms, approximation_types, penalty_scalings, iterations_list, approximation_precisions, problem_path_prefix, result_path_prefix, number_runs=number_runs, max_num_samples=max_num_samples, timeout_in_ms=timeout_in_ms)

    approximation_types = ['legacy']
    approximation_precisions = [(4, 2, [0.63]), (5, 2, [2.55]), (6, 2, [5.11])]
    conduct_benchmark_experiments(da_algorithms, approximation_types, penalty_scalings, iterations_list, approximation_precisions, problem_path_prefix, result_path_prefix, number_runs=number_runs, max_num_samples=max_num_samples, timeout_in_ms=timeout_in_ms)

    data_path_prefix = 'Experiments/Data'
    result_path_prefix = 'Experiments/Results'
    replace_existing_results = True
    approximation_types = ['quadratic', 'legacy']
    process_benchmark_annealing_results(da_algorithms, approximation_types, penalty_scalings, iterations_list, considered_thres_configs, problem_path_prefix, data_path_prefix, result_path_prefix, number_runs=number_runs, max_num_samples=max_num_samples, replace_existing_results=replace_existing_results, timeout_in_ms=timeout_in_ms)


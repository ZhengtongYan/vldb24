{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f096bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import prod\n",
    "import math\n",
    "import itertools\n",
    "from math import inf\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "import Scripts.QUBOGenerator as QUBOGenerator\n",
    "import Scripts.ProblemGenerator as ProblemGenerator\n",
    "import Scripts.Postprocessing as Postprocessing\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import time\n",
    "\n",
    "from dadk.QUBOSolverDAv2 import QUBOSolverDAv2\n",
    "from dadk.QUBOSolverCPU import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d0de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, path, filename):\n",
    "    sd = os.path.abspath(path)\n",
    "    pathlib.Path(sd).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    f = open(path + '/' + filename, 'a', newline='')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(data)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def load_data(path, filename):\n",
    "    datafile = os.path.abspath(path + '/' + filename)\n",
    "    if os.path.exists(datafile):\n",
    "        with open(datafile, 'rb') as file:\n",
    "            return json.load(file)\n",
    "        \n",
    "def load_all_results(path):\n",
    "    if not os.path.isdir(path):\n",
    "        return []\n",
    "    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    data = []\n",
    "    for datafile in onlyfiles:\n",
    "        with open(path + '/' + datafile, 'rb') as file:\n",
    "            data.append(json.load(file))\n",
    "    return data\n",
    "\n",
    "def save_data(data, path, filename):\n",
    "    datapath = os.path.abspath(path)\n",
    "    pathlib.Path(datapath).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    datafile = os.path.abspath(path + '/' + filename)\n",
    "    mode = 'a' if os.path.exists(datafile) else 'w'\n",
    "    with open(datafile, mode) as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4afd5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_problem(fujitsu_qubo, da_algorithm='annealing', number_runs=100, number_iterations=1000000, test_with_local_solver=False):\n",
    "#def solve_problem(fujitsu_qubo, number_runs=100, number_iterations=1000000, temperature_start=1000, temperature_end=10, temperature_interval=10000, test_with_local_solver=False):\n",
    "    if test_with_local_solver:\n",
    "        solver = QUBOSolverCPU(number_runs=number_runs)\n",
    "    else:\n",
    "        if da_algorithm == 'annealing':\n",
    "            solver = QUBOSolverDAv2(optimization_method=da_algorithm, timeout=60, number_iterations=number_iterations, number_runs=number_runs, access_profile_file='annealer.prf', use_access_profile=True)\n",
    "        else:\n",
    "            solver = QUBOSolverDAv2(optimization_method=da_algorithm, timeout=60, number_iterations=number_iterations, number_replicas=number_runs, access_profile_file='annealer.prf', use_access_profile=True)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            solution_list = solver.minimize(fujitsu_qubo)\n",
    "            break\n",
    "        except:\n",
    "            print(\"Library error. Repeating request\")\n",
    "\n",
    "    execution_time = solution_list.execution_time.total_seconds()\n",
    "    anneal_time = solution_list.anneal_time.total_seconds()\n",
    "    solutions = solution_list.solutions\n",
    "    return solutions, execution_time, anneal_time\n",
    "\n",
    "def parse_solutions_for_serialisation(raw_solutions):\n",
    "    response = []\n",
    "    for raw_solution in raw_solutions:\n",
    "        solution = [raw_solution.configuration, float(raw_solution.frequency), float(raw_solution.energy)]\n",
    "        response.append(solution)\n",
    "    return response\n",
    "                                                \n",
    "def conduct_synthetic_annealing_experiments3(query_types, relations, graph_types, problems, approximation_precisions, penalty_scalings, approximation_types, da_algorithms, iterations_list, problem_path_prefix, result_path_prefix, number_runs=100, number_iterations=1000000, samples = range(0, 20)):\n",
    "    \n",
    "    for query_type in query_types:    \n",
    "        for graph_type in graph_types:\n",
    "            for i in relations:\n",
    "                for j in problems:\n",
    "                    problem_path_main = str(i) + 'relations/' + graph_type + '_graph/' + str(j)\n",
    "                    card, pred, pred_sel = ProblemGenerator.get_join_ordering_problem(problem_path_prefix + '/' + query_type + '_queries/' + problem_path_main, generated_problems=True)\n",
    "\n",
    "                    for da_algorithm in da_algorithms:\n",
    "                        for penalty_scaling in penalty_scalings:\n",
    "                            for l in range(len(approximation_precisions)):\n",
    "                                (ap, num_decimal_pos, thres) = approximation_precisions[l]\n",
    "                                for approximation_type in approximation_types:\n",
    "                                    if approximation_type == 'quadratic':\n",
    "                                        fujitsu_qubo, penalty_weight = QUBOGenerator.generate_Fujitsu_QUBO_for_left_deep_trees_v3(card, pred, pred_sel, thres[0], num_decimal_pos, penalty_scaling=penalty_scaling)\n",
    "                                    elif approximation_type == 'legacy':\n",
    "                                        fujitsu_qubo, penalty_weight = QUBOGenerator.generate_legacy_Fujitsu_QUBO_for_left_deep_trees(card, pred, pred_sel, thres, num_decimal_pos)\n",
    "                                    for s in samples:\n",
    "                                        for number_iterations in iterations_list:\n",
    "                                            result_path_suffix = 'sample_' + str(s)\n",
    "                                            result_path = result_path_prefix + '/' + query_type + '_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main + \"/thres_config_\" + str(ap) + '/' + \"penalty_scaling_\" + str(penalty_scaling) + '/' + result_path_suffix\n",
    "                                            if os.path.exists(result_path + '/' + 'response.txt'):\n",
    "                                                continue\n",
    "                                            solutions, execution_time, anneal_time = solve_problem(fujitsu_qubo, number_iterations=number_iterations, number_runs=number_runs)\n",
    "                                            response = parse_solutions_for_serialisation(solutions)\n",
    "                                            save_data([response, execution_time, anneal_time], result_path, \"response.txt\")\n",
    "                                            \n",
    "                                            thres_path = result_path_prefix + '/' + query_type + '_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main + \"/thres_config_\" + str(ap)\n",
    "                                            if not os.path.exists(thres_path + '/thres_config.txt'):\n",
    "                                                save_data(thres, thres_path, 'thres_config.txt')\n",
    "                                                \n",
    "# Current version for the old QUBO (single thresholds)                                        \n",
    "def conduct_benchmark_annealing_experiments_3(query_types, approximation_precisions, penalty_scalings, approximation_types, da_algorithms, iterations_list, problem_path_prefix, result_path_prefix, number_runs=100, number_iterations=1000000, samples = range(0, 20)):\n",
    "    \n",
    "    for query_type in query_types:    \n",
    "        queries = os.listdir(path=problem_path_prefix + '/' + query_type + '_queries')\n",
    "        for query in queries:\n",
    "            problem_path_main = query\n",
    "            card, pred, pred_sel = ProblemGenerator.get_join_ordering_problem(problem_path_prefix + '/' + query_type + '_queries/' + problem_path_main, generated_problems=True)\n",
    "            # TODO: Some benchmark queries contain predicate selectivities of 0. These are currently unhandled\n",
    "            if 0.0 in pred_sel:\n",
    "                continue\n",
    "            for da_algorithm in da_algorithms:\n",
    "                for penalty_scaling in penalty_scalings:\n",
    "                    for l in range(len(approximation_precisions)):\n",
    "                        (ap, num_decimal_pos, thres) = approximation_precisions[l]\n",
    "                        for approximation_type in approximation_types:\n",
    "                            if approximation_type == 'quadratic':\n",
    "                                fujitsu_qubo, penalty_weight = QUBOGenerator.generate_Fujitsu_QUBO_for_left_deep_trees_v3(card, pred, pred_sel, thres[0], num_decimal_pos, penalty_scaling=penalty_scaling)\n",
    "                            elif approximation_type == 'legacy':\n",
    "                                fujitsu_qubo, penalty_weight = QUBOGenerator.generate_legacy_Fujitsu_QUBO_for_left_deep_trees(card, pred, pred_sel, thres, num_decimal_pos)\n",
    "                            else:\n",
    "                                continue\n",
    "                            for s in samples:\n",
    "                                for number_iterations in iterations_list:\n",
    "                                    result_path_suffix = 'sample_' + str(s)\n",
    "                                    result_path = result_path_prefix + '/' + query_type + '_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main + \"/thres_config_\" + str(ap) + \"/penalty_scaling_\" + str(penalty_scaling) + '/' + result_path_suffix\n",
    "                                    if os.path.exists(result_path + '/' + \"response.txt\"):\n",
    "                                        continue\n",
    "                                    solutions, execution_time, anneal_time = solve_problem(fujitsu_qubo, number_iterations=number_iterations, number_runs=number_runs)\n",
    "                                    response = parse_solutions_for_serialisation(solutions)\n",
    "                                    save_data([response, float(execution_time), float(anneal_time)], result_path, \"response.txt\")                        \n",
    "\n",
    "                                    thres_path = result_path_prefix + '/' + query_type + '_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main + \"/thres_config_\" + str(ap)\n",
    "                                    if not os.path.exists(thres_path + '/thres_config.txt'):\n",
    "                                        save_data(thres, thres_path, 'thres_config.txt')\n",
    "\n",
    "def export_synthetic_annealing_results_aggregated(query_types, relations, graph_types, problems, algorithms, da_algorithms, approximation_types, milp_step_sizes, approximation_precisions, penalty_scalings, iterations_list, considered_thres_configs, postprocessing_methods, problem_path_prefix, benchmark_prefix, milp_prefix, fujitsu_path_prefix, result_path, number_runs=100, samples = range(0, 20), include_header=True, include_benchmarks=True, include_milp=True, include_annealing=True, include_raw_annealing=True, include_random=True):\n",
    "    if include_header:\n",
    "        csv_data = ['method', 'use_thresholds', 'postprocessing_method', 'milp_step_size', 'query_type', 'num_relations', 'graph_type', 'problem', 'baseline_cost', 'annealing_threshold', 'penalty_scaling', 'num_iterations', 'sample', 'optimisation_time_in_ms', 'access_time_in_ms', 'shot', 'cost', 'normalised_cost']\n",
    "        save_to_csv(csv_data, result_path, 'synthetic_results_aggregated.txt')     \n",
    "    \n",
    "    start = time.time()\n",
    "    best_costs = inf\n",
    "    for query_type in query_types:\n",
    "        for graph_type in graph_types:\n",
    "            for i in relations:\n",
    "                for j in problems:\n",
    "                    csv_data_list = []\n",
    "                    baseline_cost = inf\n",
    "\n",
    "                    problem_path_main = str(i) + 'relations/' + graph_type + '_graph/' + str(j)\n",
    "                    card, pred, pred_sel = ProblemGenerator.get_join_ordering_problem(problem_path_prefix + '/' + query_type + '_queries/' + problem_path_main, generated_problems=True)\n",
    "\n",
    "                    # Process Benchmark results\n",
    "                    if include_benchmarks:\n",
    "                        for (algorithm, tree_type) in algorithms.items():\n",
    "                            jo_result = load_data(benchmark_prefix + '/' + query_type + '_queries/' + problem_path_main, algorithm + '.json')\n",
    "                            if jo_result is None:\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 0]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                                continue\n",
    "                            join_order = jo_result[0]\n",
    "                            solution_time = jo_result[1]\n",
    "                            if len(join_order) > 0:\n",
    "                                if tree_type == 'bushy':\n",
    "                                    costs = Postprocessing.get_costs_for_bushy_tree(join_order, card, pred, pred_sel)\n",
    "                                else:\n",
    "                                    costs = Postprocessing.get_costs_for_leftdeep_tree(join_order, card, pred, pred_sel, {})\n",
    "                                if costs < baseline_cost:\n",
    "                                    baseline_cost = costs\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 'n/a', solution_time, 'n/a', 'n/a', int(costs), 0]\n",
    "                            else:\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 0]\n",
    "                            csv_data_list.append(csv_data)\n",
    "                            \n",
    "                    # Process MILP results\n",
    "                    min_milp_cost = inf\n",
    "                    best_milp_result = None\n",
    "                    if include_milp:\n",
    "                        for step_size in milp_step_sizes:\n",
    "                            result = load_data(milp_prefix + '/' + query_type + '_queries/' + problem_path_main + '/' + str(step_size) + '_steps', 'order.json')\n",
    "                            if result is None:\n",
    "                                continue\n",
    "                            join_order = result[0]\n",
    "                            solution_time = result[1]\n",
    "                            if len(join_order) > 0:\n",
    "                                costs = Postprocessing.get_costs_for_leftdeep_tree(join_order, card, pred, pred_sel, {})\n",
    "                                if costs < min_milp_cost:\n",
    "                                    min_milp_cost = costs\n",
    "                                    best_milp_result = ['milp', 'n/a', 'n/a', step_size, query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 'n/a', solution_time, 'n/a', 'n/a', int(costs), 0]\n",
    "                                if costs < baseline_cost:\n",
    "                                    baseline_cost = costs\n",
    "                    \n",
    "                    if best_milp_result is not None:\n",
    "                        csv_data_list.append(best_milp_result)\n",
    "                    else:\n",
    "                        best_milp_result = ['milp', 'n/a', 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 'n/a', solution_time, 'n/a', 'n/a', 'n/a', 0]\n",
    "                        csv_data_list.append(best_milp_result)\n",
    "                            \n",
    "                    # Process Fujitsu results\n",
    "                    min_annealing_cost = inf\n",
    "                    best_annealing_result = None\n",
    "                    if include_annealing:\n",
    "                        card_dict = {}\n",
    "                        for approximation_type in approximation_types:\n",
    "                            min_annealing_cost = inf\n",
    "                            best_annealing_result = None\n",
    "                            for da_algorithm in da_algorithms:\n",
    "                                for penalty_scaling in penalty_scalings:\n",
    "                                    for number_iterations in iterations_list:\n",
    "                                        thres_config_path = fujitsu_path_prefix + '/steinbrunn_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main\n",
    "                                        if not os.path.exists(thres_config_path):\n",
    "                                            continue\n",
    "                                        thres_configs = os.listdir(path=thres_config_path)\n",
    "                                        for thres_config in thres_configs:\n",
    "                                            if considered_thres_configs is not None and thres_config not in considered_thres_configs:\n",
    "                                                continue\n",
    "                                            annealing_thresholds = load_data(thres_config_path + '/' + thres_config, 'thres_config.txt')\n",
    "                                            if len(annealing_thresholds) > 0:\n",
    "                                                annealing_threshold = annealing_thresholds[0]\n",
    "                                            else:\n",
    "                                                annealing_threshold = 0\n",
    "                                            for s in samples:\n",
    "                                                result_path_suffix = 'sample_' + str(s)\n",
    "                                                fujitsu_result_path = fujitsu_path_prefix + '/' + query_type + '_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main + \"/\" + thres_config + \"/penalty_scaling_\" + str(penalty_scaling) + '/' + result_path_suffix\n",
    "                                                response = load_data(fujitsu_result_path, \"response.txt\")\n",
    "                                                if response is None:\n",
    "                                                    continue\n",
    "                                                access_time = response[1] * 1000\n",
    "                                                solution_time = response[2] * 1000\n",
    "                                                for postprocessing_method in postprocessing_methods:\n",
    "                                                    if postprocessing_method == 1:\n",
    "                                                        best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                                    elif postprocessing_method == 2:\n",
    "                                                        best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess2(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                                    elif postprocessing_method == 3:\n",
    "                                                        best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess3(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                                    elif postprocessing_method == 4:\n",
    "                                                        best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess4(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                                    elif postprocessing_method == 5:\n",
    "                                                        best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess5(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                                    elif postprocessing_method == 6:\n",
    "                                                        best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess6(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                                    else:\n",
    "                                                        continue \n",
    "                                                    for r in range(number_runs):\n",
    "                                                        if locally_best_costs[r] < min_annealing_cost:\n",
    "                                                            min_annealing_cost = locally_best_costs[r]\n",
    "                                                            best_annealing_result = [da_algorithm + '_' + approximation_type, 'false', postprocessing_method, 'n/a', query_type, i, graph_type, j, 0, annealing_threshold, penalty_scaling, number_iterations, s, solution_time, access_time, r, int(locally_best_costs[r]), 0]\n",
    "                                                        if locally_best_costs[r] < baseline_cost:\n",
    "                                                            baseline_cost = locally_best_costs[r]\n",
    "\n",
    "\n",
    "                            if best_annealing_result is not None:\n",
    "                                csv_data_list.append(best_annealing_result) \n",
    "                            else:\n",
    "                                best_annealing_result = [da_algorithm + '_' + approximation_type, 'false', postprocessing_method, 'n/a', query_type, i, graph_type, j, 0, annealing_threshold, penalty_scaling, number_iterations, s, solution_time, access_time, r, 'n/a', 0]\n",
    "                                csv_data_list.append(best_annealing_result) \n",
    "                        \n",
    "                    # Export csv data\n",
    "                    for csv_data in csv_data_list:\n",
    "                        csv_data[8] = int(baseline_cost)\n",
    "                        if csv_data[len(csv_data)-2] != 'n/a':\n",
    "                            normalised_cost = csv_data[len(csv_data)-2]/int(baseline_cost)\n",
    "                            if normalised_cost > 50:\n",
    "                                csv_data[len(csv_data)-1] = 50\n",
    "                            else:\n",
    "                                csv_data[len(csv_data)-1] = csv_data[len(csv_data)-2]/int(baseline_cost)\n",
    "                        else:\n",
    "                            csv_data[len(csv_data)-1] = 50\n",
    "                        save_to_csv(csv_data, result_path, 'synthetic_results_aggregated.txt')\n",
    "                        \n",
    "                        \n",
    "def export_synthetic_annealing_results_aggregated_new(query_types, relations, graph_types, problems, algorithms, da_algorithms, approximation_types, milp_step_sizes, approximation_precisions, penalty_scalings, iterations_list, considered_thres_configs, problem_path_prefix, benchmark_prefix, milp_prefix, fujitsu_path_prefix, result_path, number_runs=100, samples = range(0, 20), na_cost=20, include_header=True, include_benchmarks=True, include_milp=True, include_annealing=True, include_raw_annealing=True, include_random=True):\n",
    "    if include_header:\n",
    "        csv_data = ['method', 'postprocessed', 'milp_step_size', 'query_type', 'num_relations', 'graph_type', 'problem', 'baseline_cost', 'annealing_threshold', 'penalty_scaling', 'num_iterations', 'optimisation_time_in_ms', 'access_time_in_ms', 'cost', 'normalised_cost']\n",
    "        save_to_csv(csv_data, result_path, 'synthetic_results_aggregated.txt')     \n",
    "    \n",
    "    start = time.time()\n",
    "    best_costs = inf\n",
    "    for query_type in query_types:\n",
    "        for graph_type in graph_types:\n",
    "            for i in relations:\n",
    "                for j in problems:\n",
    "                    csv_data_list = []\n",
    "                    baseline_cost = inf\n",
    "\n",
    "                    problem_path_main = str(i) + 'relations/' + graph_type + '_graph/' + str(j)\n",
    "                    card, pred, pred_sel = ProblemGenerator.get_join_ordering_problem(problem_path_prefix + '/' + query_type + '_queries/' + problem_path_main, generated_problems=True)\n",
    "\n",
    "                    # Process Benchmark results\n",
    "                    if include_benchmarks:\n",
    "                        for (algorithm, tree_type) in algorithms.items():\n",
    "                            jo_result = load_data(benchmark_prefix + '/' + query_type + '_queries/' + problem_path_main, algorithm + '.json')\n",
    "                            if jo_result is None:\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 0]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                                continue\n",
    "                            join_order = jo_result[0]\n",
    "                            solution_time = jo_result[1]\n",
    "                            if len(join_order) > 0:\n",
    "                                if tree_type == 'bushy':\n",
    "                                    costs = Postprocessing.get_costs_for_bushy_tree(join_order, card, pred, pred_sel)\n",
    "                                else:\n",
    "                                    costs = Postprocessing.get_costs_for_leftdeep_tree(join_order, card, pred, pred_sel, {})\n",
    "                                if costs < baseline_cost:\n",
    "                                    baseline_cost = costs\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', solution_time, 'n/a', int(costs), 0]\n",
    "                            else:\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 0]\n",
    "                            csv_data_list.append(csv_data)\n",
    "                            \n",
    "                    # Process MILP results\n",
    "                    min_milp_cost = inf\n",
    "                    best_milp_result = None\n",
    "                    if include_milp:\n",
    "                        for step_size in milp_step_sizes:\n",
    "                            result = load_data(milp_prefix + '/' + query_type + '_queries/' + problem_path_main + '/' + str(step_size) + '_steps/60.0_timeout', 'order.json')\n",
    "                            if result is None:\n",
    "                                continue\n",
    "                            join_order = result[0]\n",
    "                            solution_time = result[1]\n",
    "                            if len(join_order) > 0:\n",
    "                                costs = Postprocessing.get_costs_for_leftdeep_tree(join_order, card, pred, pred_sel, {})\n",
    "                                if costs < min_milp_cost:\n",
    "                                    min_milp_cost = costs\n",
    "                                    best_milp_result = ['milp', 'n/a', step_size, query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', solution_time, 'n/a', int(costs), 0]\n",
    "                                if costs < baseline_cost:\n",
    "                                    baseline_cost = costs\n",
    "                    \n",
    "                    if best_milp_result is not None:\n",
    "                        csv_data_list.append(best_milp_result)\n",
    "                    else:\n",
    "                        best_milp_result = ['milp', 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', solution_time, 'n/a', 'n/a', 0]\n",
    "                        csv_data_list.append(best_milp_result)\n",
    "                            \n",
    "                    # Process Fujitsu results\n",
    "                    min_annealing_cost = inf\n",
    "                    best_annealing_result = None\n",
    "                    if include_annealing:\n",
    "                        card_dict = {}\n",
    "                        for approximation_type in approximation_types:\n",
    "                            min_annealing_cost = inf\n",
    "                            best_annealing_result = None\n",
    "                            for da_algorithm in da_algorithms:\n",
    "                                for penalty_scaling in penalty_scalings:\n",
    "                                    for number_iterations in iterations_list:\n",
    "                                        thres_config_path = fujitsu_path_prefix + '/steinbrunn_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main\n",
    "                                        if not os.path.exists(thres_config_path):\n",
    "                                            continue\n",
    "                                        thres_configs = os.listdir(path=thres_config_path)\n",
    "                                        for thres_config in thres_configs:\n",
    "                                            if considered_thres_configs is not None and thres_config not in considered_thres_configs:\n",
    "                                                continue\n",
    "                                            annealing_thresholds = load_data(thres_config_path + '/' + thres_config, 'thres_config.txt')\n",
    "                                            if len(annealing_thresholds) > 0:\n",
    "                                                annealing_threshold = annealing_thresholds[0]\n",
    "                                            else:\n",
    "                                                annealing_threshold = 0\n",
    "                                            for s in samples:\n",
    "                                                result_path_suffix = 'sample_' + str(s)\n",
    "                                                fujitsu_result_path = fujitsu_path_prefix + '/' + query_type + '_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main + \"/\" + thres_config + \"/penalty_scaling_\" + str(penalty_scaling) + '/' + result_path_suffix\n",
    "                                                response = load_data(fujitsu_result_path, \"response.txt\")\n",
    "                                                if response is None:\n",
    "                                                    continue\n",
    "                                                access_time = response[1] * 1000\n",
    "                                                solution_time = response[2] * 1000\n",
    "                                                best_solutions_for_time, solutions = Postprocessing.readout(response, card, pred, pred_sel, card_dict)\n",
    "                                                final_solution = best_solutions_for_time[len(best_solutions_for_time)-1]\n",
    "                                                annealing_cost = final_solution[1]\n",
    "                                                if annealing_cost >= min_annealing_cost:\n",
    "                                                    continue\n",
    "                                                min_annealing_cost = annealing_cost\n",
    "                                                postprocessed = final_solution[3]\n",
    "                                                best_annealing_result = [da_algorithm + '_' + approximation_type, postprocessed, 'n/a', query_type, i, graph_type, j, 0, annealing_threshold, penalty_scaling, number_iterations, solution_time, access_time, annealing_cost, 0]\n",
    "                                                if annealing_cost < baseline_cost:\n",
    "                                                    baseline_cost = annealing_cost\n",
    "\n",
    "                            if best_annealing_result is not None:\n",
    "                                csv_data_list.append(best_annealing_result) \n",
    "                            else:\n",
    "                                best_annealing_result = [da_algorithm + '_' + approximation_type, postprocessing_method, 'n/a', query_type, i, graph_type, j, 0, annealing_threshold, penalty_scaling, number_iterations, solution_time, access_time, 'n/a', 0]\n",
    "                                csv_data_list.append(best_annealing_result) \n",
    "                    \n",
    "                    # Export csv data\n",
    "                    for csv_data in csv_data_list:\n",
    "                        csv_data[7] = int(baseline_cost)\n",
    "                        if csv_data[len(csv_data)-2] != 'n/a':\n",
    "                            normalised_cost = csv_data[len(csv_data)-2]/int(baseline_cost)\n",
    "                            if normalised_cost > na_cost:\n",
    "                                csv_data[len(csv_data)-1] = na_cost\n",
    "                            else:\n",
    "                                csv_data[len(csv_data)-1] = csv_data[len(csv_data)-2]/int(baseline_cost)\n",
    "                        else:\n",
    "                            csv_data[len(csv_data)-1] = na_cost\n",
    "                        save_to_csv(csv_data, result_path, 'synthetic_results_aggregated.txt')\n",
    "\n",
    "def export_synthetic_annealing_times(query_types, relations, graph_types, problems, algorithms, da_algorithms, approximation_types, milp_step_sizes, approximation_precisions, penalty_scalings, iterations_list, considered_thres_configs, problem_path_prefix, benchmark_prefix, milp_prefix, fujitsu_path_prefix, result_path, number_runs=100, samples = range(0, 20), timeout_in_ms=60000, na_cost=20, include_header=True, include_benchmarks=True, include_milp=True, include_annealing=True, include_raw_annealing=True, include_random=True):\n",
    "    if include_header:\n",
    "        csv_data = ['method', 'postprocessed', 'milp_step_size', 'query_type', 'num_relations', 'graph_type', 'problem', 'baseline_cost', 'thres_config', 'penalty_scaling', 'num_iterations', 'optimisation_time_in_ms', 'cost', 'normalised_cost']\n",
    "        save_to_csv(csv_data, result_path, 'synthetic_times.txt')     \n",
    "    \n",
    "    start = time.time()\n",
    "    best_costs = inf\n",
    "    for query_type in query_types:\n",
    "        for graph_type in graph_types:\n",
    "            for i in relations:\n",
    "                for j in problems:\n",
    "                    csv_data_list = []\n",
    "                    baseline_cost = inf\n",
    "\n",
    "                    problem_path_main = str(i) + 'relations/' + graph_type + '_graph/' + str(j)\n",
    "                    card, pred, pred_sel = ProblemGenerator.get_join_ordering_problem(problem_path_prefix + '/' + query_type + '_queries/' + problem_path_main, generated_problems=True)\n",
    "\n",
    "                    # Process Benchmark results\n",
    "                    if include_benchmarks:\n",
    "                        for (algorithm, tree_type) in algorithms.items():\n",
    "                            jo_result = load_data(benchmark_prefix + '/' + query_type + '_queries/' + problem_path_main, algorithm + '.json')\n",
    "                            if jo_result is None:\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 0, 'n/a', 0]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', timeout_in_ms, 'n/a', 0]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                                continue\n",
    "                            join_order = jo_result[0]\n",
    "                            solution_time = jo_result[1]\n",
    "                            if solution_time > timeout_in_ms:\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 0, 'n/a', 0]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', timeout_in_ms, 'n/a', 0]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                                continue\n",
    "                            if solution_time != 0:\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 0, 'n/a', na_cost]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                            if len(join_order) > 0:\n",
    "                                if tree_type == 'bushy':\n",
    "                                    costs = Postprocessing.get_costs_for_bushy_tree(join_order, card, pred, pred_sel)\n",
    "                                else:\n",
    "                                    costs = Postprocessing.get_costs_for_leftdeep_tree(join_order, card, pred, pred_sel, {})\n",
    "                                if costs < baseline_cost:\n",
    "                                    baseline_cost = costs\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', solution_time, int(costs), 0]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', timeout_in_ms, int(costs), 0]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                            else:\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 0, 'n/a', 0]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                                csv_data = [algorithm, 'n/a', 'n/a', query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', timeout_in_ms, 'n/a', 0]\n",
    "                                csv_data_list.append(csv_data)\n",
    "                            \n",
    "                    # Process MILP results\n",
    "                    if include_milp:\n",
    "                        milp_step_results = {}\n",
    "                        for step_size in milp_step_sizes:\n",
    "                            milp_step_results[step_size] = []\n",
    "                            milp_path = milp_prefix + '/' + query_type + '_queries/' + problem_path_main + '/' + str(step_size) + '_steps'\n",
    "                            if not os.path.exists(milp_path):\n",
    "                                continue\n",
    "                            timeout_strings = os.listdir(path=milp_path)\n",
    "                            timeouts = []\n",
    "                            for timeout_string in timeout_strings:\n",
    "                                timeout = float(timeout_string.split('_')[0])\n",
    "                                timeouts.append(timeout)\n",
    "                            timeouts = list(sorted(timeouts))\n",
    "                            if timeouts[0] != 0:\n",
    "                                milp_result = ['milp', 'n/a', step_size, query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', 0, 'n/a', na_cost]\n",
    "                                milp_step_results[step_size].append(milp_result)\n",
    "\n",
    "                            min_milp_cost = inf\n",
    "                            min_milp_result = None\n",
    "                            for timeout in timeouts:\n",
    "                                result = load_data(milp_prefix + '/' + query_type + '_queries/' + problem_path_main + '/' + str(step_size) + '_steps/' + str(timeout) + '_timeout', 'order.json')\n",
    "                                if result is None:\n",
    "                                    continue\n",
    "                                join_order = result[0]\n",
    "                                solution_time = result[1]\n",
    "                                if solution_time > timeout_in_ms:\n",
    "                                    # The MILP solver often terminates only a few ms after the timeout\n",
    "                                    # Hence, we allow up to one second time overhead\n",
    "                                    if solution_time < timeout_in_ms + 1000:\n",
    "                                        solution_time = timeout_in_ms\n",
    "                                    continue\n",
    "                                if len(join_order) > 0:\n",
    "                                    costs = Postprocessing.get_costs_for_leftdeep_tree(join_order, card, pred, pred_sel, {})\n",
    "                                    if costs < min_milp_cost:\n",
    "                                        min_milp_result = ['milp', 'n/a', step_size, query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', solution_time, int(costs), 0]\n",
    "                                        min_milp_cost = costs\n",
    "                                        milp_step_results[step_size].append(min_milp_result)\n",
    "                                    if costs < baseline_cost:\n",
    "                                        baseline_cost = costs\n",
    "                            if min_milp_result is not None:\n",
    "                                min_milp_result = min_milp_result.copy()\n",
    "                                min_milp_result[11] = timeout_in_ms\n",
    "                                milp_step_results[step_size].append(min_milp_result)\n",
    "                            else:\n",
    "                                milp_result = ['milp', 'n/a', step_size, query_type, i, graph_type, j, 0, 'n/a', 'n/a', 'n/a', timeout_in_ms, 'n/a', na_cost]\n",
    "                                milp_step_results[step_size].append(milp_result)\n",
    "                        \n",
    "                        # Export the best MILP results obtained from all step sizes\n",
    "                        # For equal cost, we prefer higher step sizes, which tend to beget lower optimisation times\n",
    "                        best_step_size = max(milp_step_sizes)\n",
    "                        min_step_cost = inf\n",
    "                        for (step_size, milp_step_result) in milp_step_results.items():\n",
    "                            final_result = milp_step_result[len(milp_step_result)-1]\n",
    "                            final_result_cost = final_result[len(final_result)-2]\n",
    "                            if final_result_cost == 'n/a':\n",
    "                                continue\n",
    "                            if final_result_cost < min_step_cost:\n",
    "                                min_step_cost = final_result_cost\n",
    "                                best_step_size = step_size\n",
    "                            elif final_result_cost == min_step_cost and step_size > best_step_size:\n",
    "                                best_step_size = step_size\n",
    "                        best_milp_results = milp_step_results[best_step_size]\n",
    "                        for best_milp_result in best_milp_results:\n",
    "                            csv_data_list.append(best_milp_result)\n",
    "                            \n",
    "                    # Process Fujitsu results\n",
    "                    if include_annealing:\n",
    "                        card_dict = {}\n",
    "                        for approximation_type in approximation_types:\n",
    "                            for da_algorithm in da_algorithms:\n",
    "                                for penalty_scaling in penalty_scalings:\n",
    "                                    for number_iterations in iterations_list:\n",
    "                                        thres_config_path = fujitsu_path_prefix + '/steinbrunn_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main\n",
    "                                        if not os.path.exists(thres_config_path):\n",
    "                                            continue\n",
    "                                        thres_configs = os.listdir(path=thres_config_path)\n",
    "                                        annealing_thres_results = {}\n",
    "                                        min_thres_cost = inf\n",
    "                                        best_thres_config = None\n",
    "                                        for thres_config in thres_configs:\n",
    "                                            if considered_thres_configs is not None and thres_config not in considered_thres_configs:\n",
    "                                                continue\n",
    "                                            if best_thres_config is None:\n",
    "                                                best_thres_config = thres_config\n",
    "                                            annealing_thres_results[thres_config] = []\n",
    "                                            annealing_result = [da_algorithm + '_' + approximation_type, 1, 'n/a', query_type, i, graph_type, j, 0, thres_config, penalty_scaling, number_iterations, 0, 'n/a', na_cost]\n",
    "                                            annealing_thres_results[thres_config].append(annealing_result)\n",
    "\n",
    "                                            solution_time = 0\n",
    "                                            min_annealing_cost = inf\n",
    "                                            min_annealing_result = None\n",
    "                                            for s in samples:\n",
    "                                                result_path_suffix = 'sample_' + str(s)\n",
    "                                                fujitsu_result_path = fujitsu_path_prefix + '/' + query_type + '_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main + \"/\" + thres_config + \"/penalty_scaling_\" + str(penalty_scaling) + '/' + result_path_suffix\n",
    "                                                response = load_data(fujitsu_result_path, \"response.txt\")\n",
    "                                                if response is None:\n",
    "                                                    continue\n",
    "                                                access_time = response[1] * 1000\n",
    "                                                solution_time = solution_time + access_time\n",
    "                                                best_solutions_for_time, solutions = Postprocessing.readout(response, card, pred, pred_sel, card_dict)\n",
    "                                                final_solution = best_solutions_for_time[len(best_solutions_for_time)-1]\n",
    "                                                    \n",
    "                                                annealing_cost = final_solution[1]\n",
    "                                                if annealing_cost >= min_annealing_cost:\n",
    "                                                    continue\n",
    "                                                min_annealing_cost = annealing_cost\n",
    "                                                readout_time = final_solution[2]\n",
    "                                                postprocessed = final_solution[3]\n",
    "                                                if (solution_time + readout_time) > timeout_in_ms:\n",
    "                                                    continue\n",
    "                                                annealing_result = [da_algorithm + '_' + approximation_type, postprocessed, 'n/a', query_type, i, graph_type, j, 0, thres_config, penalty_scaling, number_iterations, solution_time + readout_time, annealing_cost, 0]\n",
    "                                                annealing_thres_results[thres_config].append(annealing_result)  \n",
    "                                                min_annealing_result = annealing_result\n",
    "                                                if annealing_cost < min_thres_cost:\n",
    "                                                    min_thres_cost = annealing_cost\n",
    "                                                    best_thres_config = thres_config\n",
    "                                                if annealing_cost < baseline_cost:\n",
    "                                                    baseline_cost = annealing_cost\n",
    "                                            \n",
    "                                            min_annealing_result = min_annealing_result.copy()\n",
    "                                            min_annealing_result[len(min_annealing_result)-3] = timeout_in_ms\n",
    "                                            annealing_thres_results[thres_config].append(min_annealing_result)\n",
    "                                        best_annealing_results = annealing_thres_results[best_thres_config]\n",
    "                                        for best_annealing_result in best_annealing_results:\n",
    "                                            csv_data_list.append(best_annealing_result)\n",
    "\n",
    "                    # Export csv data\n",
    "                    for csv_data in csv_data_list:\n",
    "                        csv_data[7] = int(baseline_cost)\n",
    "                        if csv_data[len(csv_data)-2] != 'n/a':\n",
    "                            normalised_cost = csv_data[len(csv_data)-2]/int(baseline_cost)\n",
    "                            if normalised_cost > na_cost:\n",
    "                                csv_data[len(csv_data)-1] = na_cost\n",
    "                            else:\n",
    "                                csv_data[len(csv_data)-1] = csv_data[len(csv_data)-2]/int(baseline_cost)\n",
    "                        else:\n",
    "                            csv_data[len(csv_data)-1] = na_cost\n",
    "                        save_to_csv(csv_data, result_path, 'synthetic_times.txt')     \n",
    " \n",
    "\n",
    "# Export for multiple threshold values\n",
    "def export_benchmark_annealing_results3_2(query_types, algorithms, da_algorithms, approximation_types, milp_step_sizes, penalty_scalings, iterations_list, considered_thres_configs, postprocessing_methods, problem_path_prefix, benchmark_prefix, milp_prefix, fujitsu_path_prefix, result_path, number_runs=100, samples = range(0, 20), include_header=True, include_benchmarks=True, include_milp=True, include_annealing=True, include_raw_annealing=True, include_random=True):\n",
    "    if include_header:\n",
    "        csv_data = ['method', 'use_thresholds', 'postprocessing_method', 'milp_step_size', 'query_type', 'query', 'baseline_cost', 'thresh_config', 'penalty_scaling', 'num_iterations', 'sample', 'optimisation_time_in_ms', 'access_time_in_ms', 'cost']\n",
    "        save_to_csv(csv_data, result_path, 'benchmark_results.txt')     \n",
    "    \n",
    "    start = time.time()\n",
    "    best_costs = inf\n",
    "    for query_type in query_types:\n",
    "        queries = os.listdir(path=problem_path_prefix + '/' + query_type + '_queries')\n",
    "        for query in queries:\n",
    "            query_number = int(query.split('q')[1])\n",
    "            problem_path_main = query\n",
    "        \n",
    "            csv_data_list = []\n",
    "            baseline_cost = inf\n",
    "\n",
    "            card, pred, pred_sel = ProblemGenerator.get_join_ordering_problem(problem_path_prefix + '/' + query_type + '_queries/' + problem_path_main, generated_problems=True)\n",
    "      \n",
    "            # Process Benchmark results\n",
    "            if include_benchmarks:\n",
    "                for (algorithm, tree_type) in algorithms.items():\n",
    "                    jo_result = load_data(benchmark_prefix + '/' + query_type + '_queries/' + problem_path_main, algorithm + '.json')\n",
    "                    if jo_result is None:\n",
    "                        continue\n",
    "                    join_order = jo_result[0]\n",
    "                    solution_time = jo_result[1]\n",
    "                    if len(join_order) > 0:\n",
    "                        if tree_type == 'bushy':\n",
    "                            costs = Postprocessing.get_costs_for_bushy_tree(join_order, card, pred, pred_sel)\n",
    "                        else:\n",
    "                            costs = Postprocessing.get_costs_for_leftdeep_tree(join_order, card, pred, pred_sel, {})\n",
    "                        #print(costs)\n",
    "                        if costs < baseline_cost:\n",
    "                            baseline_cost = costs\n",
    "                        csv_data = [algorithm, 'n/a', 'n/a', 'n/a', query_type, query_number, 0, 'n/a', 'n/a', 'n/a', 'n/a', solution_time, 'n/a', int(costs)]\n",
    "                    else:\n",
    "                        csv_data = [algorithm, 'n/a', 'n/a', 'n/a', query_type, query_number, 0, 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a']\n",
    "                    csv_data_list.append(csv_data)\n",
    "                            \n",
    "            # Process MILP results\n",
    "            min_milp_cost = inf\n",
    "            best_milp_result = None\n",
    "            if include_milp:\n",
    "                for step_size in milp_step_sizes:\n",
    "                    result = load_data(milp_prefix + '/' + query_type + '_queries/' + problem_path_main + '/' + str(step_size) + '_steps', 'order.json')\n",
    "                    if result is None:\n",
    "                        continue\n",
    "                    join_order = result[0]\n",
    "                    solution_time = result[1]\n",
    "                    if len(join_order) > 0:\n",
    "                        costs = Postprocessing.get_costs_for_leftdeep_tree(join_order, card, pred, pred_sel, {})\n",
    "                        if costs < min_milp_cost:\n",
    "                            min_milp_cost = costs\n",
    "                            best_milp_result = ['milp', 'n/a', 'n/a', step_size, query_type, query_number, 0, 'n/a', 'n/a', 'n/a', 'n/a', solution_time, 'n/a', int(costs)]\n",
    "                        if costs < baseline_cost:\n",
    "                            baseline_cost = costs\n",
    "            \n",
    "            if best_milp_result is not None:\n",
    "                #best_milp_result = ['milp', 'n/a', 'n/a', step_size, query_type, query, 0, 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a', 'n/a']\n",
    "                csv_data_list.append(best_milp_result)\n",
    "                                    \n",
    "            # Process Fujitsu results\n",
    "            if include_annealing:\n",
    "                card_dict = {}\n",
    "                for approximation_type in approximation_types:\n",
    "                    min_annealing_cost = inf\n",
    "                    best_annealing_result = None\n",
    "                    for da_algorithm in da_algorithms:\n",
    "                        for penalty_scaling in penalty_scalings:\n",
    "                            for number_iterations in iterations_list:\n",
    "                                thres_config_path = fujitsu_path_prefix + '/' + query_type + '_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main\n",
    "                                if not os.path.exists(thres_config_path):\n",
    "                                    continue\n",
    "                                thres_configs = os.listdir(path=thres_config_path)\n",
    "                                for thres_config in thres_configs:\n",
    "                                    if considered_thres_configs is not None and thres_config not in considered_thres_configs:\n",
    "                                        continue\n",
    "                                    for s in samples:\n",
    "                                        result_path_suffix = 'sample_' + str(s)\n",
    "                                        fujitsu_result_path = fujitsu_path_prefix + '/' + query_type + '_queries/' + da_algorithm + '/' + approximation_type + '_approximation/' + str(number_iterations) + '_iterations/' + str(number_runs) + '_shots/' + problem_path_main + \"/\" + thres_config + \"/penalty_scaling_\" + str(penalty_scaling) + '/' + result_path_suffix\n",
    "                                        response = load_data(fujitsu_result_path, \"response.txt\")\n",
    "                                        if response is None:\n",
    "                                            continue\n",
    "                                        access_time = response[1] * 1000\n",
    "                                        solution_time = response[2] * 1000\n",
    "                                        for postprocessing_method in postprocessing_methods:\n",
    "                                            if postprocessing_method == 1:\n",
    "                                                best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                            elif postprocessing_method == 2:\n",
    "                                                best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess2(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                            elif postprocessing_method == 3:\n",
    "                                                best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess3(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                            elif postprocessing_method == 4:\n",
    "                                                best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess4(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                            elif postprocessing_method == 5:\n",
    "                                                best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess5(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                            elif postprocessing_method == 6:\n",
    "                                                best_order, best_costs, min_index, optimal_cost_count, locally_best_costs = Postprocessing.postprocess6(response, card, pred, pred_sel, card_dict, best_costs, start)\n",
    "                                            else:\n",
    "                                                continue\n",
    "                                            for r in range(number_runs):\n",
    "                                                if locally_best_costs[r] < min_annealing_cost:\n",
    "                                                    min_annealing_cost = locally_best_costs[r]\n",
    "                                                    best_annealing_result = [da_algorithm + '_' + approximation_type, 'false', postprocessing_method, 'n/a', query_type, query_number, 0, thres_config, penalty_scaling, number_iterations, s, solution_time, access_time, int(locally_best_costs[r])]\n",
    "                                                if locally_best_costs[r] < baseline_cost:\n",
    "                                                    baseline_cost = locally_best_costs[r]\n",
    "            \n",
    "                    if best_annealing_result is not None:\n",
    "                        csv_data_list.append(best_annealing_result)     \n",
    "\n",
    "            # Export csv data\n",
    "            for csv_data in csv_data_list:\n",
    "                csv_data[6] = int(baseline_cost)\n",
    "                save_to_csv(csv_data, result_path, 'benchmark_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e8b8bcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def conduct_experiment():\n",
    "    query_types = ['steinbrunn']\n",
    "    #query_types = ['sqlite']\n",
    "    \n",
    "    relations = [18, 22, 26, 30, 34, 38, 42, 46, 50]\n",
    "    \n",
    "    graph_types = ['CHAIN', 'STAR', 'CYCLE']\n",
    "    \n",
    "    problems = range(10)\n",
    "\n",
    "    penalty_scalings = [2]\n",
    "            \n",
    "    iterations_list = [1000000]\n",
    "    \n",
    "    problem_path_prefix = 'ExperimentalAnalysis/Fujitsu/Problems'\n",
    "    fujitsu_path_prefix = 'ExperimentalAnalysis/Fujitsu/Results'\n",
    "    benchmark_prefix = 'ExperimentalAnalysis/Benchmarks/Results'\n",
    "    milp_prefix = 'ExperimentalAnalysis/MILP/Results'\n",
    "    result_path = 'ExperimentalAnalysis'\n",
    "    algorithms = {'ikkbz': 'leftdeep', 'dpsizelinear': 'leftdeep'}\n",
    "    \n",
    "    da_algorithms = ['annealing']\n",
    "    \n",
    "    milp_step_sizes = [2, 10, 100]\n",
    "    \n",
    "    thres_configs = ['thres_config_4', 'thres_config_5', 'thres_config_6', 'thres_config_12']\n",
    "    \n",
    "    postprocessing_methods = [1, 2]\n",
    "    \n",
    "    number_runs = 100\n",
    "    samples = range(10)\n",
    "    \n",
    "    include_header = True\n",
    "    include_benchmarks = True\n",
    "    include_milp = True\n",
    "    include_annealing = True\n",
    "    include_raw_annealing = True\n",
    "    include_random = False\n",
    "    aggregate_annealing_results = True\n",
    "    \n",
    "    approximation_types = ['quadratic']\n",
    "    \n",
    "    # Synthetic experiments\n",
    "    approximation_precisions = [(4, 2, [0.63])]\n",
    "    export_synthetic_annealing_results_aggregated_new(query_types, relations, graph_types, problems, algorithms, da_algorithms, approximation_types, milp_step_sizes, approximation_precisions, penalty_scalings, iterations_list, thres_configs, problem_path_prefix, benchmark_prefix, milp_prefix, fujitsu_path_prefix, result_path, number_runs=number_runs, samples = samples, include_header=include_header, include_benchmarks=include_benchmarks, include_milp=include_milp, include_annealing=include_annealing, include_raw_annealing=include_raw_annealing, include_random=include_random)\n",
    "    #export_synthetic_annealing_times(query_types, relations, graph_types, problems, algorithms, da_algorithms, approximation_types, milp_step_sizes, approximation_precisions, penalty_scalings, iterations_list, thres_configs, problem_path_prefix, benchmark_prefix, milp_prefix, fujitsu_path_prefix, result_path, number_runs=number_runs, samples = samples, include_header=include_header, include_benchmarks=include_benchmarks, include_milp=include_milp, include_annealing=include_annealing, include_raw_annealing=include_raw_annealing, include_random=include_random)\n",
    "    \n",
    "    # Benchmark experiments\n",
    "    approximation_precisions = [(12, 0, [])]\n",
    "    #conduct_benchmark_annealing_experiments_3(query_types, approximation_precisions, penalty_scalings, approximation_types, da_algorithms, iterations_list, problem_path_prefix, fujitsu_path_prefix, number_runs=number_runs, samples = samples)\n",
    "    #export_benchmark_annealing_results3_2(query_types, algorithms, da_algorithms, approximation_types, milp_step_sizes, penalty_scalings, iterations_list, thres_configs, postprocessing_methods, problem_path_prefix, benchmark_prefix, milp_prefix, fujitsu_path_prefix, result_path, number_runs=number_runs, samples = samples, include_header=include_header, include_benchmarks=include_benchmarks, include_milp=include_milp, include_annealing=include_annealing, include_raw_annealing=include_raw_annealing, include_random=include_random)\n",
    "\n",
    "conduct_experiment()\n",
    "\n",
    "#from dadk.JupyterTools import CleanupJobs\n",
    "#CleanupJobs(access_profile_file='annealer.prf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
